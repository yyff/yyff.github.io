---
layout: article
title:  "文本分类初探"
categories: other
excerpt: "本篇文章是关于我的中文文本分类的学习总结与实践"
---


> 文章欢迎转载，但转载时请保留本段文字，并置于文章的顶部 作者：c0ks 本文原文地址：http://yyff.github.io/other/text-clf/

{% include toc.html %}


## 0x0 整体步骤

![dm-step]({{ site.url }}/images/other/text-clf/dm1.jpg)

## 0x1 数据预处理

### 聚集（aggregation）
聚集将两个或多个数据对象合并成单个对象。

### 抽样
抽样是选择数据对象子集的常用方法。 

* 简单随机抽样（1）有放回抽样 （2）无放回抽样
* 分层抽样
* 渐进（自适应）抽样

### 维规约
数据集中可能包含大量特征， 维规约可以删除不相关的特征并降低噪声。

* 主成分分析（PCA）：是一种用于连续属性的线性代数技术，它找出新的属性（主成分），这些属性是原属性的线性组合，是相互正交的
* 奇异值分解（SVD）

### 特征子集选择(feature seletction)
降低维度的另一种方法是仅使用特征的一个子集。

1. 方法
	* 嵌入方法：在数据挖掘（机器学习）算法运行期间决定使用哪些属性和忽略哪些属性，如构造决策树分类器时通常以这种方式运行
	* 过滤方法：在数据挖掘（机器学习）算法运行前进行特征选择
	* 包装方法：用数据挖掘（机器学习）算法枚举找出最佳特征子集

2. 特征子集选择体系

	![feature-select-step]({{ site.url }}/images/other/text-clf/feature-select-step.png)

3. 特征加权
特征加权（如word-count、 tf、tf-idf等）是另一种保留和删除特征的办法，例如根据阈值去除权值低的特征。
> 通常来说，特征加权被用在特征抽取(feature extraction)而不是特征选择(feature selection)中， 但有时用在特征选择中也能产生很好的效果。

### 特征创建
1. 特征提取（feature extration）：从原始数据集创建新的特征集
	
	> [特征提取vs特征选择](http://blog.csdn.net/j123kaishichufa/article/details/7679682)
	 
2. 映射数据到新的空间
	
	例如对有大龄周期模式的时间序列数据实施[傅里叶变换](https://zh.wikipedia.org/wiki/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2)，将它转换成频率信息明显的表示，就能检测到这些模型
	
3. 特征构造
	有时原始数据集的特征具有必要的信息，但其形式不适合数据挖掘算法，此时用一个或多个原始特征构造的新特征可能比元特征更有用。

### 离散化或二元化

1. 离散化：将连续属性变换成离散的分类属性（例如把分数[0,100]划分成不及格，良好，优秀）
2. 二元化：将连续或离散的分类属性变换成二元属性（例如不及格，良好，优秀变换成不及格、及格）

## 0x2 分类器

1. [最近邻分类器（knn）](https://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95)：是一种用于分类和回归的非参数统计方法。
2. [贝叶斯分类（Naive Bayes）](https://zh.wikipedia.org/zh-cn/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8)：朴素贝叶斯分类器是一系列以假设特征之间强（朴素）独立下运用贝叶斯定理为基础的简单概率分类器。
3. [支持向量机（SVM）](https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA)：是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。


## 0x3 实践

* 数据集：20M豆瓣影评（）
* 分词：[jieba](https://github.com/fxsjy/jieba)
* 分类器算法：[scikit-learn](http://scikit-learn.org/)

### 程序
1.预处理：分词、去标点、分类属性二元化

```python
def cut_words(line):
	words = list(jieba.cut(line.strip(), cut_all = True))
	# 去除jieba分词留下了标点符号
	words = [filterpunt(word).lower() for word in words if not (len(word) == 1 and word.isalpha()) and word not in stop_words and len(filterpunt(word)) > 0]
	return words
	
# 预处理：分词、去标点、分类属性二元化
def preprocess(raw_data):
	
	labels = []
	data = []
	label_data_dict = {} # 标签对应的数据集，key:label, value:此label对应的所有data下标形成的list
	
	for line in raw_data:
		if line[0] == '(' and line[1].isdigit() and line[5] == ')':
			label = int(line[1])
			if label == 0:
				continue;
			words = cut_words(line[7:])
			if len(words) > 0:
				#分类属性二元化
				if label <= 2:
					label = 0 # negtive
				else:
					label = 1 # positive
	
				labels.append(label)
				data.append(words)
				if label not in label_data_dict:
					label_data_dict[label] = []
				label_data_dict[label].append(len(data) - 1)
	
	return data, labels, label_data_dict
``` 
2.划分训练、测试集：切割数据集使各类别的数据量相同（使用有最小数据量的类别作为各类别的数据量），然后将每个类别的向量集根据ratio提供的比例划分成训练集和测试集

```python
min_label_datasize = 0
for key in label_data_dict.keys():
	if min_label_datasize == 0 or len(label_data_dict[key]) < min_label_datasize:
		min_label_datasize = len(label_data_dict[key])
	
for key in label_data_dict.keys():
	label_data_dict[key] = label_data_dict[key][:min_label_datasize]
	
print "data size of every class:", min_label_datasize
	
train_data, train_label, test_data, test_label = divide_dataset(data, labels, label_data_dict, ratio=0.7)
	
```

3.特征提取及向量化

	[sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)提供了四种方法进行特征提取。

```python
# 3.特征提取及向量化
print '************** 1. CountVectorizer **************'  
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
# 数据集已经过分词，analyzer直接返回原向量即可， 根据max_features阶段词汇表
count_vectorizer = CountVectorizer(analyzer = lambda x : x, max_features=10000) 
# fit_transform = fit + transform, fit: 训练模型（生成词汇表等） transform：将词向量转变成数值向量 
# 只对训练集fit， 对训练集和测试集transform
train_features = count_vectorizer.fit_transform(train_data) 
test_features = count_vectorizer.transform(test_data)

print '************** 2. CountVectorizer + TfidfTransformer **************'  
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
count_vectorizer = CountVectorizer(analyzer = lambda x : x, max_features=10000) 
train_features = count_vectorizer.fit_transform(train_data)
test_features = count_vectorizer.transform(test_data)
# 用tf-idf对word count向量进行加权
tfidftransformer = TfidfTransformer()
train_features = tfidftransformer.fit_transform(train_features)
test_features = tfidftransformer.transform(test_features)

print '**************************** 3. TfidfVectorizer ****************************'  
from sklearn.feature_extraction.text import TfidfVectorizer  
tfidf_vectorizer = TfidfVectorizer(sublinear_tf = True,  
                                   max_df = 0.5,
                                   analyzer = lambda x : x);  
train_features = tfidf_vectorizer.fit_transform(train_data);  
test_features = tfidf_vectorizer.transform(test_data);  

print '**************************** 4. HashingVectorizer ****************************'  
from sklearn.feature_extraction.text import HashingVectorizer  
hash_vectorizer = HashingVectorizer(analyzer = lambda x : x, non_negative = True,  
                               n_features = 10000)  
train_features = hash_vectorizer.fit_transform(train_data)  
test_features = hash_vectorizer.transform(test_data);  	
```
	
4.分类器

```python
from sklearn.metrics import classification_report # for report
# 4. 训练和评估分类模型
print '************** Naive Bayes **************'  
from sklearn.naive_bayes import MultinomialNB  
nb_clf = MultinomialNB(alpha = 0.01)   
nb_clf.fit(train_features, train_label)
pred = nb_clf.predict(test_features)
print(classification_report(test_label, pred))


print '************** KNN **************'  
from sklearn.neighbors import KNeighborsClassifier 
knn_clf = KNeighborsClassifier()   
knn_clf.fit(train_features, train_label)
pred = knn_clf.predict(test_features)
print(classification_report(test_label, pred))

print '************** SVM **************'  
from sklearn.svm import SVC 
svm_clf = SVC(kernel = 'linear') #default kernel function is rbf  
svm_clf.fit(train_features, train_label)  
pred = svm_clf.predict(test_features);  
print(classification_report(test_label, pred))
```

> 完整的程序和数据集：[github](https://github.com/yyff/MLexercise/clf)
	
### 评估

特征提取上使用CountVectorizer + TfidfTransformer、分类器使用Naive Bayes 获得了较高的分类效果：

```
************** Naive Bayes **************
             precision    recall  f1-score   support

          0       0.74      0.84      0.79      5517
          1       0.82      0.70      0.75      5517

avg / total       0.78      0.77      0.77     11034

************** KNN **************
             precision    recall  f1-score   support

          0       0.67      0.77      0.72      5517
          1       0.73      0.62      0.67      5517

avg / total       0.70      0.69      0.69     11034

************** SVM **************
             precision    recall  f1-score   support

          0       0.73      0.86      0.79      5517
          1       0.83      0.68      0.75      5517

avg / total       0.78      0.77      0.77     11034
```

## 0x4 Reference
1. 《数据挖掘导论》
2. [应用scikit-learn做文本分类](http://blog.csdn.net/abcjennifer/article/details/23615947)
